#!/usr/bin/python3
# -*- coding: utf-8 -*-
# @file   : node_prediction.py
# @author : Zhutian Lin
# @date   : 2019/10/17
# @version: 1.0
# @desc   : å®Œæˆç‚¹åˆ†ç±»ä»»åŠ?
import pandas as pd
import numpy as np
import logging
import sklearn
from gensim.models import Word2Vec
from sklearn.svm import SVC
from sklearn.metrics import f1_score
from gensim.models import KeyedVectors
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
LOG_FORMAT = "%(asctime)s - %(levelname)s - %(message)s"    # æ—¥å¿—æ ¼å¼åŒ–è¾“å‡?
DATE_FORMAT = "%m/%d/%Y %H:%M:%S %p"                        # æ—¥æœŸæ ¼å¼
fp = logging.FileHandler('config.log', encoding='utf-8')
fs = logging.StreamHandler()
logging.basicConfig(level=logging.DEBUG, format=LOG_FORMAT, datefmt=DATE_FORMAT, handlers=[fs, fp])    # è°ƒç”¨handlers=[fp,fs]
#  è¾“å‡ºåˆ°å‘½ä»¤è¡Œä¸?
#  è®¡æ—¶å™?
import time
import sys


class node_classilier:
    def __init__(self):
        logging.info("åˆå§‹åŒ–åˆ†ç±»å™¨")
        self.vectors_dict = {}  # æ¯ä¸ªç‚¹å¯¹åº”çš„å­—å…¸{ç‚¹index(str):å‘é‡(nparray)} è¿™ä¸ªstrå¯ä¸èƒ½å¸¦å°æ•°ç‚?
        self.node_dict = {}  # æ¯ä¸ªç‚¹çš„label
        self.node_list = []  # å¤‡é€‰ç‚¹
        self.X_train = []
        self.X_test = []
        self.y_train = []
        self.y_test = []
    # æ“ä½œï¼šé¦–å…ˆå¯¼å…¥æ•°æ®é›†ï¼Œæ ¼å¼ä¸å˜ï¼›åŸæ¥çš„test_setè¾“å…¥ä¸ºä»¥ç©ºæ ¼ä¸ºåˆ†éš”ç¬¦çš„node1 node2ï¼Œä¸»ç¨‹åºçš„embeddingï¼Œæ ¼å¼ä¸å?

    def import_model(self, model_name, option=0):
        #  0ä»£è¡¨ä¸»æ–¹æ³•ï¼Œmodel_nameä»£è¡¨çš„æ˜¯ä¸»å‡½æ•°è¾“å‡ºçš„èŠ‚ç‚¹å‘é‡
        if option == 0:
            word_vectors = np.loadtxt(model_name, delimiter=' ')
            for line in word_vectors:
                tmp = {str(int(line[0])): line[1:-1]}
                self.vectors_dict.update(tmp)
            self.node_list = [node for node in self.vectors_dict]

        #  1ä»£è¡¨onlineï¼Œmodel_nameä»£è¡¨çš„æ˜¯onlineä¿å­˜çš„sgæ¨¡å‹
        if option == 1:
            model = Word2Vec.load(model_name)
            word_vectors = KeyedVectors.load(model_name)
            # æ„é€ æ–°å­—å…¸
            for key in word_vectors.wv.vocab.keys():
                tmp = {key: model.wv.__getitem__(key)}
                self.vectors_dict.update(tmp)
            self.node_list = [node for node in self.vectors_dict]

        #  2ä»£è¡¨ä½¿ç”¨çš„æ˜¯æ ‡å‡†sgï¼Œmodel_nameä»£è¡¨çš„æ˜¯æ ‡å‡†sgä¿ç•™çš„å‘é‡ï¼ˆé¦–è¡Œä¸ºèŠ‚ç‚¹æ•°å’Œç»´åº¦å¤§å°ï¼‰
        if option == 2:
            word_vectors = KeyedVectors.load_word2vec_format(model_name)
            self.node_list = word_vectors.wv.vocab.keys()
            for node_id in self.node_list:
                tmp = {node_id: word_vectors[node_id]}
                # tmp = {node_id: list(word_vectors[node_id])}
                self.vectors_dict.update(tmp)

        #  3ä»£è¡¨è‡ªå·±ç”Ÿæˆçš„å‘é‡ï¼ŒèŠ‚ç‚¹ç¬¬ä¸€ä½ä¸ºå­—æ¯è¡¨ç¤º
        if option == 3:
            f = open(model_name)
            for line in f:
                toks = line.strip().split(' ')
                if toks.__len__() < 3:
                    continue
                tmp = {toks[0][1:]: list(map(float, toks[1:]))}
                self.vectors_dict.update(tmp)
            self.node_list = [node for node in self.vectors_dict]
            f.close()

    def import_node(self, dsname):
        # ç»Ÿä¸€æ“ä½œä¸ºstr
        # node_data = np.loadtxt(dsname, delimiter="\t").tolist()
        # node_data = np.loadtxt(dsname, delimiter=",").tolist()
        node_data = np.loadtxt(dsname, dtype=str, delimiter="\t")
        # node_data = np.loadtxt(dsname, dtype=str, delimiter="\t").tolist()
        # node_data = np.loadtxt(dsname, dtype=str, delimiter=",").tolist()
        for edge in node_data:
            tmp_1 = {edge[0]: edge[1]}
            tmp_2 = {edge[2]: edge[3]}
            # tmp_1 = {str(int(edge[0])): str(int(edge[1]))}
            # tmp_2 = {str(int(edge[2])): str(int(edge[3]))}
            self.node_dict.update(tmp_1)
            self.node_dict.update(tmp_2)

        node_data = []

    def build_train_test(self):
        logging.debug("build_train_test")
        vec = []
        label = []
        for node in self.vectors_dict:
            node_vec = self.vectors_dict.get(node)
            if type(node_vec) != list:
                node_vec = node_vec.tolist()
            # node_vec = node_vec.tolist()
            if node in self.vectors_dict and node in self.node_dict:  # å»æ‰æ²¡æœ‰å‹ä¸­
                vec.append(node_vec)
                label.append(self.node_dict.get(node))
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(vec, label, test_size=0.25)

    def classify(self):
        logging.debug("classify")
        w = []
        a = []
        i = []
        

        lr = LogisticRegression(C=1000.0, random_state=0)
        lr.fit(self.X_train, self.y_train)
        Y_predict_lr = lr.predict(self.X_test).tolist()
        mif1_lr = f1_score(self.y_test, Y_predict_lr, average='micro')
        maf1_lr = f1_score(self.y_test, Y_predict_lr, average='macro')
        f1_lr = f1_score(self.y_test, Y_predict_lr, average='weighted')
        
        # print("ç»è¿‡lrè®­ç»ƒçš„é¢„æµ‹æƒ…å†µä¸ºï¼?+str(f1_lr))
        print("weighted  "+str(f1_lr))
        print("micro  "+str(mif1_lr))
        print("macro  "+str(maf1_lr))

        w.append(str(f1_lr))
        a.append(str(maf1_lr))
        i.append(str(mif1_lr))
          
        with open('result.txt','a') as f:
            f.write('/'.join(w)+':weighted\n')
            f.write('/'.join(a)+':macro\n')
            f.write('/'.join(i)+':micro\n')
        # svm = SVC(kernel='linear', C=1.0, random_state=0)
        # svm.fit(self.X_train, self.y_train)
        # Y_predict_svm = svm.predict(self.X_test).tolist()
        # f1_svm = f1_score(self.y_test,Y_predict_svm,average='weighted')
        # print("ç»è¿‡svmè®­ç»ƒçš„é¢„æµ‹æƒ…å†µä¸ºï¼?+str(f1_svm))


if __name__ == '__main__':
    # experiment
    # python node_classification.py ../output/withJUST/TKY180/emb/vec_final.emb ../dataset/TKY/train_graph.txt 0
    # python node_classification.py ../output/withJUST/TKY190/emb/vec_final.emb ../dataset/TKY/train_graph.txt 0

    # python node_classification.py ../output/withJUST/TKY190_full/emb/vec_final.emb ../dataset/TKY/full_graph.txt 0
    # python node_classification.py ../output/withJUST/TKY190_full_shuffle/emb/vec_final.emb ../dataset/TKY/full_graph.txt 0
    # python node_classification.py ../output/withJUST/TKY190_full_init_in_window/emb/vec_final.emb ../dataset/TKY/full_graph.txt 0
    # python node_classification.py ../output/withJUST/TKY190_full_no_init/emb/vec_final.emb ../dataset/TKY/full_graph.txt 0
    # python node_classification.py ../output/withJUST/TKY190_full_p1/emb/vec_final.emb ../dataset/TKY/full_graph.txt 0
    # python node_classification.py ../output/withJUST/TKY190_full_p5/emb/vec_final.emb ../dataset/TKY/full_graph.txt 0
    # python node_classification.py ../output/withJUST/TKY190_full_p0/emb/vec_final.emb ../dataset/TKY/full_graph.txt 0

    # æ¶ˆèå®éªŒï¼?
    # python node_classification.py ../baseline/ablation_experiment/output/TRWwithoutJUST/TKY190_full/emb/vec_final.emb ../dataset/TKY/full_graph.txt 0
    # python node_classification.py ../baseline/ablation_experiment/output/JUSTwithoutTRW/TKY190_full/emb/vec_final.emb ../dataset/TKY/full_graph.txt 2
    # python node_classification.py ../baseline/ablation_experiment/output/JUST_static/TKY_555437/emb/vec_init.emb ../dataset/TKY/full_graph.txt 2
    # python node_classification.py ../baseline/ablation_experiment/output/modelwithStdSG/TKY190_full/emb/vec_final.emb ../dataset/TKY/full_graph.txt 0
    # baselineå®éªŒï¼?
    # python node_classification.py ../baseline/output/dynamic_sg_gjw/TKY190_full/emb/555436.emb ../dataset/TKY/full_graph.txt 2
    # python node_classification.py ../baseline/output/change2vec_gjw/TKY190_full/emb_merge/vec_final.emb ../dataset/TKY/full_graph.txt 3
    # python node_classification.py ../baseline/CTDNE/output/TKY_new_w10/model/54_model ../dataset/TKY/full_graph.txt 1
    # python node_classification.py ../baseline/output/metapath2vec/TKY/metapath_wholemodel ../dataset/TKY/full_graph.txt 1
    # python node_classification.py ../baseline/output/metapath2vec/TKY/l40_whole_metapath_model ../dataset/TKY/full_graph.txt 1

    # enron:
    # python node_classification.py ../output/withJUST/enron190_skip5/without_na_others/emb/vec_final.emb ../dataset/sig_enron/enron_dele_na_others.txt 0
    # python node_classification.py ../output/withJUST/enron190_skip5_shuffle/without_na_others/emb/vec_final.emb ../dataset/sig_enron/enron_dele_na_others.txt 0
    # python node_classification.py ../output/withJUST/enron190_skip5_init_in_window/without_na_others/emb/vec_final.emb ../dataset/sig_enron/enron_dele_na_others.txt 0
    # python node_classification.py ../output/withJUST/enron190_skip5_no_init/without_na_others/emb/vec_final.emb ../dataset/sig_enron/enron_dele_na_others.txt 0
    # python node_classification.py ../output/withJUST/enron190_skip5_fast_no_init/without_na_others/emb/20.emb ../dataset/sig_enron/enron_dele_na_others.txt 0
    # æ¶ˆèå®éªŒï¼?
    # python node_classification.py ../baseline/ablation_experiment/output/TRWwithoutJUST/enron190_skip5_full/emb/vec_final.emb ../dataset/sig_enron/enron_dele_na_others.txt 0
    # todo:python node_classification.py ../baseline/ablation_experiment/output/JUSTwithoutTRW/enron190_skip5_full/emb/???.emb ../dataset/sig_enron/enron_dele_na_others.txt 2
    # python node_classification.py ../baseline/ablation_experiment/output/JUST_static/enron/emb/vec_init.emb ../dataset/sig_enron/enron_dele_na_others.txt 2
    # python node_classification.py ../baseline/ablation_experiment/output/modelwithStdSG/enron190_skip5_full/emb/vec_final.emb ../dataset/sig_enron/enron_dele_na_others.txt 0
    # baselineå®éªŒï¼?
    # todo:python node_classification.py ../baseline/output/dynamic_sg_gjw/enron190_skip5_full/emb/???.emb ../dataset/sig_enron/enron_dele_na_others.txt 2
    # fixme:python node_classification.py ../baseline/output/change2vec_gjw/enron190_skip5_full/emb_merge/20.emb ../dataset/sig_enron/enron_dele_na_others.txt 3
    # python node_classification.py ../baseline/output/change2vec_gjw/enron190_skip5_full_has0/emb/20.emb ../dataset/sig_enron/enron_dele_na_others.txt 3
    # python node_classification.py ../baseline/output/change2vec_gjw/enron190_skip5_full_has0/emb_merge/20.emb ../dataset/sig_enron/enron_dele_na_others.txt 3
    # python node_classification.py ../baseline/output/metapath2vec/enron/enron_20_metapath/model/metapath_model ../dataset/sig_enron/enron_dele_na_others.txt 1
    # python node_classification.py ../baseline/output/metapath2vec/enron/enron_20metapath_model ../dataset/sig_enron/enron_dele_na_others.txt 1
    # python node_classification.py ../baseline/output/metapath2vec/enron/l40_enron_20metapath_model ../dataset/sig_enron/enron_dele_na_others.txt 1
    # python node_classification.py ../baseline/output/metapath2vec/enron/metapath_model_other ../dataset/sig_enron/enron_dele_na_others.txt 1

    # python node_classification.py ../baseline/CTDNE/output/enron_full2_w5/3_model ../dataset/sig_enron/enron_dele_na_others.txt 1
    # python node_classification.py ../baseline/CTDNE/output/enron_full2/3_model ../dataset/sig_enron/enron_dele_na_others.txt 1
    # python node_classification.py ../baseline/CTDNE/output/enron_full2_w1/3_model ../dataset/sig_enron/enron_dele_na_others.txt 1
    # python node_classification.py ../baseline/CTDNE/output/enron_new_w5/model/3_model ../dataset/sig_enron/enron_dele_na_others.txt 1


    model_path = sys.argv[1]
    node_path = sys.argv[2]
    isModel = int(sys.argv[3])  # 0è¡¨ç¤ºè‡ªå·±ç”Ÿæˆçš„å‘é‡è¡¨ç¤ºï¼Œ1è¡¨ç¤ºç”±æ¨¡å‹å‡½æ•°saveçš„æ¨¡å‹è¡¨ç¤ºï¼Œ2è¡¨ç¤ºæ ‡å‡†sgæ¨¡å‹saveçš„å‘é‡è¡¨ç¤ºï¼Œ3ä»£è¡¨è‡ªå·±ç”Ÿæˆçš„é¦–ä½å­—æ¯å‘é‡?

    nlf = node_classilier()
    nlf.import_model(model_path, isModel)
    nlf.import_node(node_path)
    # nlf.import_model("29_model", 1)
    # nlf.import_node("train_graph.txt")
    # nlf.import_model("../output/bib190/emb/vec_final.emb", 0)
    # nlf.import_node("../dataset/bibsonomy-2ui/bibsonomy-2ui_75percent_tag2int.txt")
    # nlf.import_model("../output/withJUST/TKY180/emb/vec_final.emb", 0)
    # nlf.import_node("../dataset/TKY/train_graph.txt")
    nlf.build_train_test()
    nlf.classify()
